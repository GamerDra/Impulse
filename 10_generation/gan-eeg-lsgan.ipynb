{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10495954,"sourceType":"datasetVersion","datasetId":6498595}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:38:36.877011Z","iopub.execute_input":"2025-01-18T13:38:36.877440Z","iopub.status.idle":"2025-01-18T13:38:40.102129Z","shell.execute_reply.started":"2025-01-18T13:38:36.877407Z","shell.execute_reply":"2025-01-18T13:38:40.101413Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"https://arxiv.org/abs/1611.04076v3","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:38:40.103122Z","iopub.execute_input":"2025-01-18T13:38:40.103439Z","iopub.status.idle":"2025-01-18T13:38:40.159289Z","shell.execute_reply.started":"2025-01-18T13:38:40.103418Z","shell.execute_reply":"2025-01-18T13:38:40.158331Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_path = \"/kaggle/input/impulse/Impulse/EEG_Data/train_data\"\nlatent_dim = 100\ninput_dim = 19 * 500\nnum_classes = 4\nbatch_size = 128\nepochs = 100\nlearning_rate = 0.0001\nn_critic = 1  # Discriminator steps per generator step","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:38:40.160932Z","iopub.execute_input":"2025-01-18T13:38:40.161271Z","iopub.status.idle":"2025-01-18T13:38:40.179847Z","shell.execute_reply.started":"2025-01-18T13:38:40.161241Z","shell.execute_reply":"2025-01-18T13:38:40.178961Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def normalize_data(data):\n    min_val = np.min(data)\n    max_val = np.max(data)\n    return 2 * (data - min_val) / (max_val - min_val + 1e-8) - 1  # Normalize to [-1, 1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:38:40.181097Z","iopub.execute_input":"2025-01-18T13:38:40.181353Z","iopub.status.idle":"2025-01-18T13:38:40.198325Z","shell.execute_reply.started":"2025-01-18T13:38:40.181333Z","shell.execute_reply":"2025-01-18T13:38:40.197541Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class_map = {\"Normal\": 0, \"Complex_Partial_Seizures\": 1, \"Electrographic_Seizures\": 2, \"Video_detected_Seizures_with_no_visual_change_over_EEG\": 3}\ntrain_data, train_labels = [], []\n\nfor class_name, class_label in class_map.items():\n    class_folder = os.path.join(train_path, class_name)\n    for file_name in os.listdir(class_folder):\n        file_path = os.path.join(class_folder, file_name)\n        signal = np.load(file_path)  # Assuming .npy files\n        # Ensure correct shape\n        if signal.shape == (19, 500):\n            normalized_signal = normalize_data(signal)\n            train_data.append(normalized_signal.flatten())  # Flatten to shape (19*500,)\n            train_labels.append(class_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:38:40.199156Z","iopub.execute_input":"2025-01-18T13:38:40.199362Z","iopub.status.idle":"2025-01-18T13:39:09.022260Z","shell.execute_reply.started":"2025-01-18T13:38:40.199344Z","shell.execute_reply":"2025-01-18T13:39:09.021592Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Convert to tensors\ntrain_data = torch.tensor(np.array(train_data), dtype=torch.float32)\ntrain_labels = torch.tensor(train_labels, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:39:09.023115Z","iopub.execute_input":"2025-01-18T13:39:09.023433Z","iopub.status.idle":"2025-01-18T13:39:09.288777Z","shell.execute_reply.started":"2025-01-18T13:39:09.023401Z","shell.execute_reply":"2025-01-18T13:39:09.287991Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Dataset and Dataloader\nclass EEGDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:39:09.289658Z","iopub.execute_input":"2025-01-18T13:39:09.289957Z","iopub.status.idle":"2025-01-18T13:39:09.295017Z","shell.execute_reply.started":"2025-01-18T13:39:09.289927Z","shell.execute_reply":"2025-01-18T13:39:09.294297Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"dataset = EEGDataset(train_data, train_labels)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:42:16.475962Z","iopub.execute_input":"2025-01-18T13:42:16.476371Z","iopub.status.idle":"2025-01-18T13:42:16.480937Z","shell.execute_reply.started":"2025-01-18T13:42:16.476340Z","shell.execute_reply":"2025-01-18T13:42:16.480021Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim, num_classes):\n        super(Generator, self).__init__()\n        self.class_emb = nn.Sequential(\n            nn.Linear(num_classes, 16),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        input_dim = z_dim + 16\n\n        self.net = nn.Sequential(\n            # Layer 1: Upsample from 1 -> 2\n            nn.ConvTranspose1d(input_dim, 256, kernel_size=4, stride=2, padding=1),  # Output: (256, 2)\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # Layer 2: Upsample from 2 -> 8\n            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=4, padding=0),  # Output: (128, 8)\n            nn.BatchNorm1d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # Layer 3: Upsample from 8 -> 32\n            nn.ConvTranspose1d(128, 64, kernel_size=6, stride=4, padding=1),  # Output: (64, 32)\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # Layer 4: Upsample from 32 -> 128\n            nn.ConvTranspose1d(64, 32, kernel_size=8, stride=4, padding=2),  # Output: (32, 128)\n            nn.BatchNorm1d(32),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # Layer 5: Upsample from 128 -> 511\n            nn.ConvTranspose1d(32, 19, kernel_size=9, stride=4, padding=3),  # Output: (19, 511)\n            nn.Tanh()\n        )\n\n    def forward(self, z, labels):\n        # Concatenate latent vector with class embedding\n        class_emb = self.class_emb(labels)  # (batch_size, 16)\n        z = torch.cat([z, class_emb], dim=1)  # (batch_size, z_dim + 16)\n        z = z.unsqueeze(2)  # Add temporal dimension: (batch_size, z_dim + 16, 1)\n        return self.net(z)  # Output: (batch_size, 19, 511)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:46:47.526659Z","iopub.execute_input":"2025-01-18T13:46:47.526973Z","iopub.status.idle":"2025-01-18T13:46:47.535714Z","shell.execute_reply.started":"2025-01-18T13:46:47.526950Z","shell.execute_reply":"2025-01-18T13:46:47.534561Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, num_classes):\n        super(Discriminator, self).__init__()\n        self.class_emb = nn.Sequential(\n            nn.Linear(num_classes, 16),  # Embed class into 16 features\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        # Use kernel_size=3, stride=1, and padding=1 to preserve dimensions as much as possible\n        self.net = nn.Sequential(\n            nn.Conv1d(35, 64, kernel_size=3, stride=2, padding=1),  # Reduces sequence length by 2\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),  # Further reduces sequence length\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv1d(128, 1, kernel_size=3, stride=1, padding=1),  # Output is a single feature per time step\n        )\n\n    def forward(self, x, labels):\n        \"\"\"\n        x:      (batch_size, 19, time_length)\n        labels: (batch_size, num_classes)\n        \"\"\"\n        # Embed labels and expand to match temporal dimension\n        class_emb = self.class_emb(labels)  # (batch_size, 16)\n        class_emb = class_emb.unsqueeze(2).repeat(1, 1, x.size(2))  # (batch_size, 16, time_length)\n\n        # Concatenate class embedding with input EEG along channel dimension\n        x = torch.cat([x, class_emb], dim=1)  # (batch_size, 35, time_length)\n\n        # Pass through convolutional layers\n        validity = self.net(x)  # (batch_size, 1, time_length / 4 if stride=2)\n        return validity.view(x.size(0), -1)  # Flatten to (batch_size, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:46:47.537115Z","iopub.execute_input":"2025-01-18T13:46:47.537472Z","iopub.status.idle":"2025-01-18T13:46:47.559651Z","shell.execute_reply.started":"2025-01-18T13:46:47.537435Z","shell.execute_reply":"2025-01-18T13:46:47.558848Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"################################\n#      INITIALIZE MODELS       #\n################################\ngenerator = Generator(latent_dim, num_classes).to(device)\ndiscriminator = Discriminator(num_classes).to(device)\n\noptimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n\nscheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=200, gamma=0.5)\nscheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=200, gamma=0.5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:46:47.561032Z","iopub.execute_input":"2025-01-18T13:46:47.561278Z","iopub.status.idle":"2025-01-18T13:46:47.587700Z","shell.execute_reply.started":"2025-01-18T13:46:47.561258Z","shell.execute_reply":"2025-01-18T13:46:47.587000Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"################################\n#           TRAINING           #\n################################\nfor epoch in range(1, epochs + 1):\n    epoch_d_loss = 0.0\n    epoch_g_loss = 0.0\n\n    for i, (real_eeg, real_lbls) in enumerate(dataloader):\n        # Move data to device\n        real_eeg = real_eeg.to(device)  # (batch_size, 19*500) if not reshaped yet\n        real_lbls = torch.nn.functional.one_hot(real_lbls, num_classes=num_classes).float().to(device)\n\n        # Reshape real EEG to (batch_size, 19, 500) if you truly want that shape\n        # WARNING: This shape must match how your conv layers handle data\n        bsz = real_eeg.size(0)\n        real_eeg = real_eeg.view(bsz, 19, 500)\n\n        ##################################################\n        # (1) Train Discriminator (n_critic times)       #\n        ##################################################\n        for _ in range(n_critic):\n            optimizer_D.zero_grad()\n\n            # Generate fake data\n            z = torch.randn(bsz, latent_dim).to(device)\n            fake_labels = real_lbls  # Reuse the same label distribution\n            fake_eeg = generator(z, fake_labels).detach()  # (bsz, 19, ???)\n\n            # LSGAN losses\n            # Note: .detach() so these gradients don't flow back to generator\n            loss_real = 0.5 * torch.mean((discriminator(real_eeg, real_lbls) - 1) ** 2)\n            loss_fake = 0.5 * torch.mean(discriminator(fake_eeg, fake_labels) ** 2)\n            d_loss = loss_real + loss_fake\n\n            d_loss.backward()\n            optimizer_D.step()\n\n        ##################################################\n        # (2) Train Generator                            #\n        ##################################################\n        optimizer_G.zero_grad()\n\n        # Generate fake data again (w/o .detach())\n        fake_eeg = generator(z, fake_labels)\n        # LSGAN generator loss\n        g_loss = 0.5 * torch.mean((discriminator(fake_eeg, fake_labels) - 1) ** 2)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        # Accumulate losses\n        epoch_d_loss += d_loss.item()\n        epoch_g_loss += g_loss.item()\n\n    # Average the losses over the dataset\n    epoch_d_loss /= len(dataloader)\n    epoch_g_loss /= len(dataloader)\n\n    # Step schedulers if you like\n    scheduler_D.step()\n    scheduler_G.step()\n\n    print(f\"Epoch [{epoch}/{epochs}] | D_loss: {epoch_d_loss:.4f} | G_loss: {epoch_g_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:46:47.588670Z","iopub.execute_input":"2025-01-18T13:46:47.588900Z","iopub.status.idle":"2025-01-18T13:48:32.927339Z","shell.execute_reply.started":"2025-01-18T13:46:47.588880Z","shell.execute_reply":"2025-01-18T13:48:32.926299Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/100] | D_loss: 0.3875 | G_loss: 0.3507\nEpoch [2/100] | D_loss: 0.2463 | G_loss: 0.1694\nEpoch [3/100] | D_loss: 0.1973 | G_loss: 0.1697\nEpoch [4/100] | D_loss: 0.2257 | G_loss: 0.1493\nEpoch [5/100] | D_loss: 0.2498 | G_loss: 0.1348\nEpoch [6/100] | D_loss: 0.2454 | G_loss: 0.1339\nEpoch [7/100] | D_loss: 0.2420 | G_loss: 0.1330\nEpoch [8/100] | D_loss: 0.2396 | G_loss: 0.1343\nEpoch [9/100] | D_loss: 0.2322 | G_loss: 0.1376\nEpoch [10/100] | D_loss: 0.2317 | G_loss: 0.1382\nEpoch [11/100] | D_loss: 0.2339 | G_loss: 0.1373\nEpoch [12/100] | D_loss: 0.2335 | G_loss: 0.1362\nEpoch [13/100] | D_loss: 0.2411 | G_loss: 0.1322\nEpoch [14/100] | D_loss: 0.2448 | G_loss: 0.1303\nEpoch [15/100] | D_loss: 0.2496 | G_loss: 0.1285\nEpoch [16/100] | D_loss: 0.2502 | G_loss: 0.1275\nEpoch [17/100] | D_loss: 0.2497 | G_loss: 0.1272\nEpoch [18/100] | D_loss: 0.2487 | G_loss: 0.1274\nEpoch [19/100] | D_loss: 0.2484 | G_loss: 0.1279\nEpoch [20/100] | D_loss: 0.2481 | G_loss: 0.1278\nEpoch [21/100] | D_loss: 0.2481 | G_loss: 0.1273\nEpoch [22/100] | D_loss: 0.2481 | G_loss: 0.1277\nEpoch [23/100] | D_loss: 0.2480 | G_loss: 0.1279\nEpoch [24/100] | D_loss: 0.2479 | G_loss: 0.1276\nEpoch [25/100] | D_loss: 0.2480 | G_loss: 0.1272\nEpoch [26/100] | D_loss: 0.2481 | G_loss: 0.1274\nEpoch [27/100] | D_loss: 0.2486 | G_loss: 0.1272\nEpoch [28/100] | D_loss: 0.2495 | G_loss: 0.1262\nEpoch [29/100] | D_loss: 0.2497 | G_loss: 0.1261\nEpoch [30/100] | D_loss: 0.2492 | G_loss: 0.1267\nEpoch [31/100] | D_loss: 0.2487 | G_loss: 0.1268\nEpoch [32/100] | D_loss: 0.2482 | G_loss: 0.1269\nEpoch [33/100] | D_loss: 0.2482 | G_loss: 0.1268\nEpoch [34/100] | D_loss: 0.2476 | G_loss: 0.1273\nEpoch [35/100] | D_loss: 0.2471 | G_loss: 0.1271\nEpoch [36/100] | D_loss: 0.2470 | G_loss: 0.1273\nEpoch [37/100] | D_loss: 0.2475 | G_loss: 0.1270\nEpoch [38/100] | D_loss: 0.2479 | G_loss: 0.1268\nEpoch [39/100] | D_loss: 0.2473 | G_loss: 0.1273\nEpoch [40/100] | D_loss: 0.2474 | G_loss: 0.1273\nEpoch [41/100] | D_loss: 0.2473 | G_loss: 0.1273\nEpoch [42/100] | D_loss: 0.2474 | G_loss: 0.1275\nEpoch [43/100] | D_loss: 0.2477 | G_loss: 0.1271\nEpoch [44/100] | D_loss: 0.2480 | G_loss: 0.1270\nEpoch [45/100] | D_loss: 0.2481 | G_loss: 0.1270\nEpoch [46/100] | D_loss: 0.2482 | G_loss: 0.1269\nEpoch [47/100] | D_loss: 0.2483 | G_loss: 0.1266\nEpoch [48/100] | D_loss: 0.2484 | G_loss: 0.1267\nEpoch [49/100] | D_loss: 0.2482 | G_loss: 0.1270\nEpoch [50/100] | D_loss: 0.2482 | G_loss: 0.1269\nEpoch [51/100] | D_loss: 0.2478 | G_loss: 0.1270\nEpoch [52/100] | D_loss: 0.2476 | G_loss: 0.1273\nEpoch [53/100] | D_loss: 0.2474 | G_loss: 0.1276\nEpoch [54/100] | D_loss: 0.2472 | G_loss: 0.1275\nEpoch [55/100] | D_loss: 0.2476 | G_loss: 0.1273\nEpoch [56/100] | D_loss: 0.2476 | G_loss: 0.1273\nEpoch [57/100] | D_loss: 0.2476 | G_loss: 0.1271\nEpoch [58/100] | D_loss: 0.2481 | G_loss: 0.1272\nEpoch [59/100] | D_loss: 0.2486 | G_loss: 0.1268\nEpoch [60/100] | D_loss: 0.2484 | G_loss: 0.1270\nEpoch [61/100] | D_loss: 0.2487 | G_loss: 0.1272\nEpoch [62/100] | D_loss: 0.2485 | G_loss: 0.1270\nEpoch [63/100] | D_loss: 0.2486 | G_loss: 0.1271\nEpoch [64/100] | D_loss: 0.2480 | G_loss: 0.1274\nEpoch [65/100] | D_loss: 0.2487 | G_loss: 0.1270\nEpoch [66/100] | D_loss: 0.2485 | G_loss: 0.1273\nEpoch [67/100] | D_loss: 0.2479 | G_loss: 0.1272\nEpoch [68/100] | D_loss: 0.2483 | G_loss: 0.1272\nEpoch [69/100] | D_loss: 0.2486 | G_loss: 0.1274\nEpoch [70/100] | D_loss: 0.2481 | G_loss: 0.1272\nEpoch [71/100] | D_loss: 0.2481 | G_loss: 0.1275\nEpoch [72/100] | D_loss: 0.2481 | G_loss: 0.1274\nEpoch [73/100] | D_loss: 0.2477 | G_loss: 0.1275\nEpoch [74/100] | D_loss: 0.2481 | G_loss: 0.1277\nEpoch [75/100] | D_loss: 0.2487 | G_loss: 0.1271\nEpoch [76/100] | D_loss: 0.2482 | G_loss: 0.1277\nEpoch [77/100] | D_loss: 0.2476 | G_loss: 0.1276\nEpoch [78/100] | D_loss: 0.2478 | G_loss: 0.1279\nEpoch [79/100] | D_loss: 0.2477 | G_loss: 0.1279\nEpoch [80/100] | D_loss: 0.2477 | G_loss: 0.1281\nEpoch [81/100] | D_loss: 0.2481 | G_loss: 0.1276\nEpoch [82/100] | D_loss: 0.2477 | G_loss: 0.1277\nEpoch [83/100] | D_loss: 0.2480 | G_loss: 0.1279\nEpoch [84/100] | D_loss: 0.2483 | G_loss: 0.1276\nEpoch [85/100] | D_loss: 0.2481 | G_loss: 0.1275\nEpoch [86/100] | D_loss: 0.2474 | G_loss: 0.1283\nEpoch [87/100] | D_loss: 0.2475 | G_loss: 0.1280\nEpoch [88/100] | D_loss: 0.2476 | G_loss: 0.1278\nEpoch [89/100] | D_loss: 0.2475 | G_loss: 0.1281\nEpoch [90/100] | D_loss: 0.2479 | G_loss: 0.1278\nEpoch [91/100] | D_loss: 0.2476 | G_loss: 0.1280\nEpoch [92/100] | D_loss: 0.2475 | G_loss: 0.1280\nEpoch [93/100] | D_loss: 0.2475 | G_loss: 0.1280\nEpoch [94/100] | D_loss: 0.2473 | G_loss: 0.1281\nEpoch [95/100] | D_loss: 0.2469 | G_loss: 0.1280\nEpoch [96/100] | D_loss: 0.2471 | G_loss: 0.1282\nEpoch [97/100] | D_loss: 0.2469 | G_loss: 0.1283\nEpoch [98/100] | D_loss: 0.2470 | G_loss: 0.1285\nEpoch [99/100] | D_loss: 0.2465 | G_loss: 0.1287\nEpoch [100/100] | D_loss: 0.2467 | G_loss: 0.1282\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Assuming generator, latent_dim, num_classes, and device are already defined\noutput_dir = \"./output\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Calculate class distribution in training data\nclass_counts = np.bincount(train_labels.numpy())\ntotal_samples = len(train_labels)\nclass_ratios = class_counts / total_samples\n\n# Set the total number of synthetic samples to generate\ntotal_synthetic_samples = 5608  # Adjust as needed\nsynthetic_samples_per_class = (class_ratios * total_synthetic_samples).astype(int)\n\n# Generate synthetic EEG data while maintaining class ratio\ngenerator.eval()\nfor class_idx, num_samples in enumerate(synthetic_samples_per_class):\n    if num_samples == 0:  # Skip classes with no samples\n        continue\n\n    z = torch.randn(num_samples, latent_dim).to(device)\n    class_label = torch.zeros(num_samples, num_classes).to(device)\n    class_label[:, class_idx] = 1  # One-hot encode the class label\n\n    with torch.no_grad():\n        synthetic_eeg = generator(z, class_label)\n        synthetic_eeg = synthetic_eeg[:, :, :500]  # Trim to (batch_size, 19, 500)\n        # Save synthetic EEG data class-wise\n        output_file = os.path.join(output_dir, f\"synthetic_eeg_{class_idx}.npy\")\n        np.save(output_file, synthetic_eeg.cpu().numpy())\n        print(f\"Saved synthetic EEG data for class {class_idx} to {output_file}\")\n\ntotal_samples\n\nsynthetic_samples_per_class","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:48:32.928532Z","iopub.execute_input":"2025-01-18T13:48:32.928877Z","iopub.status.idle":"2025-01-18T13:48:33.395897Z","shell.execute_reply.started":"2025-01-18T13:48:32.928836Z","shell.execute_reply":"2025-01-18T13:48:33.395083Z"}},"outputs":[{"name":"stdout","text":"Saved synthetic EEG data for class 0 to ./output/synthetic_eeg_0.npy\nSaved synthetic EEG data for class 1 to ./output/synthetic_eeg_1.npy\nSaved synthetic EEG data for class 2 to ./output/synthetic_eeg_2.npy\nSaved synthetic EEG data for class 3 to ./output/synthetic_eeg_3.npy\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"array([2783, 2196,  545,   84])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"synthetic_eeg.cpu().numpy().shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:48:33.397276Z","iopub.execute_input":"2025-01-18T13:48:33.397587Z","iopub.status.idle":"2025-01-18T13:48:33.403461Z","shell.execute_reply.started":"2025-01-18T13:48:33.397563Z","shell.execute_reply":"2025-01-18T13:48:33.402692Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"(84, 19, 500)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# import numpy as np\n# from scipy.linalg import sqrtm\n\n# def calculate_fid(real_features, generated_features):\n#     # Calculate mean and covariance of real features\n#     mu_r = np.mean(real_features, axis=0)\n#     sigma_r = np.cov(real_features, rowvar=False)\n    \n#     # Calculate mean and covariance of generated features\n#     mu_g = np.mean(generated_features, axis=0)\n#     sigma_g = np.cov(generated_features, rowvar=False)\n    \n#     # Calculate squared difference of means\n#     diff = mu_r - mu_g\n#     mean_diff = np.sum(diff**2)\n    \n#     # Compute square root of product of covariance matrices\n#     covmean, _ = sqrtm(sigma_r @ sigma_g, disp=False)\n    \n#     # Handle numerical errors (non-positive semi-definite results)\n#     if np.iscomplexobj(covmean):\n#         covmean = covmean.real\n    \n#     # Calculate FID score\n#     fid = mean_diff + np.trace(sigma_r + sigma_g - 2 * covmean)\n#     return fid\n\n# # Example usage\n# # real_features: Extracted features from real EEG samples (numpy array)\n# # generated_features: Extracted features from generated EEG samples (numpy array)\n\n# fid_score = calculate_fid(real_features, generated_features)\n# print(f\"FID Score: {fid_score}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:48:33.404407Z","iopub.execute_input":"2025-01-18T13:48:33.404682Z","iopub.status.idle":"2025-01-18T13:48:33.418434Z","shell.execute_reply.started":"2025-01-18T13:48:33.404660Z","shell.execute_reply":"2025-01-18T13:48:33.417556Z"}},"outputs":[],"execution_count":32}]}